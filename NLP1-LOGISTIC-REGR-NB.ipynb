{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 1</a>\n",
    "\n",
    "## Logistic Regression Model for a Classification Problem: Classify Product Reviews as Positive or Negative\n",
    "\n",
    "In this notebook, we use the Pytorch library to build a logistic regression model as a single layer neural network. Then, we will classify product reviews as positive or negative (similar to the final project dataset that you will see).\n",
    "\n",
    "\n",
    "1. <a href=\"#1\">Reading the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory data analysis</a>\n",
    "3. <a href=\"#3\">Text Processing: Stop words removal and stemming</a>\n",
    "4. <a href=\"#4\">Train - Validation Split</a>\n",
    "5. <a href=\"#5\">Data processing with Pipeline</a>\n",
    "6. <a href=\"#6\">Train the classifier</a>\n",
    "7. <a href=\"#7\">Make predictions on the validation data and test the classifier</a>\n",
    "8. <a href=\"#8\">Getting predictions on the test data and saving results</a>\n",
    "9. <a href=\"#9\">Ideas for improvement</a>\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes). *This field is a processed version of the votes field. People can click on the \"helpful\" button when they find a customer review helpful. This increases the vote by 1. __log_votes__ is calculated like this log(1+votes). This formulation helps us get a smaller range for votes.*\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch==1.8.1\n",
      "  Using cached torch-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (804.1 MB)\n",
      "Collecting torchtext==0.9.1\n",
      "  Using cached torchtext-0.9.1-cp36-cp36m-manylinux1_x86_64.whl (7.1 MB)\n",
      "Requirement already satisfied: nltk==3.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 3)) (3.6.2)\n",
      "Requirement already satisfied: pandas==1.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 4)) (1.1.5)\n",
      "Collecting scikit-learn==0.24.1\n",
      "  Using cached scikit_learn-0.24.1-cp36-cp36m-manylinux2010_x86_64.whl (22.2 MB)\n",
      "Requirement already satisfied: numpy==1.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 6)) (1.19.5)\n",
      "Collecting trax==1.3.7\n",
      "  Using cached trax-1.3.7-py2.py3-none-any.whl (521 kB)\n",
      "Collecting transformers==4.5.1\n",
      "  Using cached transformers-4.5.1-py3-none-any.whl (2.1 MB)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 1)) (3.10.0.2)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 2)) (4.61.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2.26.0)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (2021.4.4)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (8.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 5)) (2.1.0)\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.4.0-py3-none-any.whl (4.0 MB)\n",
      "Collecting funcsigs\n",
      "  Using cached funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
      "Collecting tensorflow-text\n",
      "  Using cached tensorflow_text-2.6.0-cp36-cp36m-manylinux1_x86_64.whl (4.4 MB)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-1.0.0-py3-none-any.whl (126 kB)\n",
      "Collecting gym\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "Collecting jaxlib\n",
      "  Using cached jaxlib-0.1.69-cp36-none-manylinux2010_x86_64.whl (46.5 MB)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (5.8.0)\n",
      "Collecting t5\n",
      "  Using cached t5-0.9.3-py3-none-any.whl (153 kB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting jax\n",
      "  Using cached jax-0.2.17.tar.gz (693 kB)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (4.8.1)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.0.12)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (21.0)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Using cached tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gym->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.6.0)\n",
      "Collecting opt_einsum\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (3.3)\n",
      "Collecting seqio\n",
      "  Using cached seqio-0.0.7-py3-none-any.whl (286 kB)\n",
      "Collecting tfds-nightly\n",
      "  Using cached tfds_nightly-4.4.0.dev202112080110-py3-none-any.whl (4.1 MB)\n",
      "Collecting sacrebleu\n",
      "  Using cached sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "Collecting editdistance\n",
      "  Using cached editdistance-0.6.0-cp36-cp36m-manylinux2010_x86_64.whl (284 kB)\n",
      "Requirement already satisfied: babel in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.9.1)\n",
      "Collecting rouge-score\n",
      "  Using cached rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
      "  Using cached mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from mesh-tensorflow[transformer]>=0.1.13->t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.18.2)\n",
      "Collecting portalocker\n",
      "  Using cached portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.8.9)\n",
      "Requirement already satisfied: colorama in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.4.3)\n",
      "Collecting termcolor\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting tensorflow-metadata\n",
      "  Using cached tensorflow_metadata-1.2.0-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.19.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.3.4)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-5.4.0-py3-none-any.whl (28 kB)\n",
      "Collecting promise\n",
      "  Using cached promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.53.0)\n",
      "Collecting absl-py\n",
      "  Using cached absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "Collecting tensorflow<2.7,>=2.6.0\n",
      "  Downloading tensorflow-2.6.2-cp36-cp36m-manylinux2010_x86_64.whl (458.3 MB)\n",
      "\u001b[K     |████████████████████▋           | 295.9 MB 107.0 MB/s eta 0:00:02    |██████████                      | 143.4 MB 75.5 MB/s eta 0:00:05"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 458.3 MB 7.1 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-hub>=0.8.0\n",
      "  Using cached tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
      "Collecting clang~=5.0\n",
      "  Downloading clang-5.0.tar.gz (30 kB)\n",
      "Collecting keras<2.7,>=2.6.0\n",
      "  Using cached keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
      "Collecting grpcio<2.0,>=1.37.0\n",
      "  Using cached grpcio-1.42.0-cp36-cp36m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
      "Collecting tensorboard<2.7,>=2.6.0\n",
      "  Using cached tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.1.0)\n",
      "Collecting six\n",
      "  Using cached six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.36.2)\n",
      "Collecting typing-extensions\n",
      "  Using cached typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.12.1)\n",
      "Collecting flatbuffers<3.0,>=1.12\n",
      "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast==0.4.0\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-estimator<2.7,>=2.6.0\n",
      "  Using cached tensorflow_estimator-2.6.0-py2.py3-none-any.whl (462 kB)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.2.0)\n",
      "Collecting astunparse~=1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.5.2)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.3.6-py3-none-any.whl (97 kB)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (58.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.30.2)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (4.2.2)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
      "Building wheels for collected packages: gym, jax, promise, clang, termcolor\n",
      "  Building wheel for gym (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.21.0-py3-none-any.whl size=1616066 sha256=7309fa4444930f44b04d1b8fb07d51e03bf2e028e15ae117009b87e89a6136b5\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f8/c7/3c/7ad569d779e750220d17a44f731411f0ad79b7b123a2e3a02e\n",
      "  Building wheel for jax (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jax: filename=jax-0.2.17-py3-none-any.whl size=802604 sha256=0d40c1e47abd63dd4433db7821d2d26e73b3201326585553007e5fad3c72b04f\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c0/04/e9/96543974a96ee70858fe2d8e36dbe24dcf77b32f206ec01112\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21590 sha256=3c141200433fc4663bdc238e0a35e3751b597088daedf375b0169d3519c3c6a7\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/59/9a/1d/3f1afbbb5122d0410547bf9eb50955f4a7a98e53a6d8b99bd1\n",
      "  Building wheel for clang (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for clang: filename=clang-5.0-py3-none-any.whl size=30873 sha256=6006b720ff47b171de42d6d6c540254a9d8d9120e03316d0e992deff94ab435d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/4c/94/0583f60c9c5b6024ed64f290cb2d43b06bb4f75577dc3c93a7\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4869 sha256=096f1e508b6c74a9cc3ccbdcc19a521b6af678137b21e3671fe4807a355e2422\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/93/2a/eb/e58dbcbc963549ee4f065ff80a59f274cc7210b6eab962acdc\n",
      "Successfully built gym jax promise clang termcolor\n",
      "Installing collected packages: typing-extensions, six, oauthlib, requests-oauthlib, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, termcolor, tensorflow-estimator, tensorboard, opt-einsum, keras-preprocessing, keras, gast, flatbuffers, clang, astunparse, tensorflow-metadata, tensorflow-hub, tensorflow, promise, importlib-resources, gin-config, tokenizers, tfds-nightly, tensorflow-text, tensorflow-datasets, sentencepiece, sacremoses, portalocker, mesh-tensorflow, transformers, torch, seqio, scikit-learn, sacrebleu, rouge-score, editdistance, t5, jaxlib, jax, gym, funcsigs, trax, torchtext\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.2\n",
      "    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "\u001b[31mERROR: Cannot uninstall six 1.16.0, RECORD file not found. You might be able to recover from this via: 'pip install --force-reinstall --no-deps six==1.16.0'.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Upgrade dependencies\n",
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk, re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from os import path\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import BCELoss\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Reading the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will first download the dataset and then use the __pandas__ library to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataset is: (56000, 7)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TRAINING.csv\")\n",
    "\n",
    "print(f\"The shape of the dataset is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the first 10 rows of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65886</td>\n",
       "      <td>Purchased as a quick fix for a needed Server 2...</td>\n",
       "      <td>Easy install, seamless migration</td>\n",
       "      <td>True</td>\n",
       "      <td>1458864000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19822</td>\n",
       "      <td>So far so good. Installation was simple. And r...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1417478400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14558</td>\n",
       "      <td>Microsoft keeps making Visual Studio better. I...</td>\n",
       "      <td>This is the best development tool I've ever used.</td>\n",
       "      <td>False</td>\n",
       "      <td>1252886400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39708</td>\n",
       "      <td>Very good product.</td>\n",
       "      <td>Very good product.</td>\n",
       "      <td>True</td>\n",
       "      <td>1458604800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8015</td>\n",
       "      <td>So very different from my last version and I a...</td>\n",
       "      <td>... from my last version and I am having a gre...</td>\n",
       "      <td>True</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>44530</td>\n",
       "      <td>The course is easy to follow, explicit instruc...</td>\n",
       "      <td>Well worth the money!</td>\n",
       "      <td>False</td>\n",
       "      <td>1448496000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19648</td>\n",
       "      <td>Microsoft claims Streets &amp; Trips 2008 is compa...</td>\n",
       "      <td>Microsoft Lies : Vista sucks : How to use it a...</td>\n",
       "      <td>False</td>\n",
       "      <td>1231891200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3360</td>\n",
       "      <td>The user interface is clunky. For example, the...</td>\n",
       "      <td>Too bad it isn't better; also, installation pr...</td>\n",
       "      <td>False</td>\n",
       "      <td>1335398400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>25378</td>\n",
       "      <td>They will automatically bill you for $179 afte...</td>\n",
       "      <td>Beware signing up for free 3 months online</td>\n",
       "      <td>False</td>\n",
       "      <td>1167350400</td>\n",
       "      <td>2.397895</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20006</td>\n",
       "      <td>Download the free complete version from Corel ...</td>\n",
       "      <td>Mutiple crashes</td>\n",
       "      <td>False</td>\n",
       "      <td>1259452800</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  65886  Purchased as a quick fix for a needed Server 2...   \n",
       "1  19822  So far so good. Installation was simple. And r...   \n",
       "2  14558  Microsoft keeps making Visual Studio better. I...   \n",
       "3  39708                                 Very good product.   \n",
       "4   8015  So very different from my last version and I a...   \n",
       "5  44530  The course is easy to follow, explicit instruc...   \n",
       "6  19648  Microsoft claims Streets & Trips 2008 is compa...   \n",
       "7   3360  The user interface is clunky. For example, the...   \n",
       "8  25378  They will automatically bill you for $179 afte...   \n",
       "9  20006  Download the free complete version from Corel ...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                   Easy install, seamless migration      True  1458864000   \n",
       "1                                         Five Stars      True  1417478400   \n",
       "2  This is the best development tool I've ever used.     False  1252886400   \n",
       "3                                 Very good product.      True  1458604800   \n",
       "4  ... from my last version and I am having a gre...      True  1454716800   \n",
       "5                              Well worth the money!     False  1448496000   \n",
       "6  Microsoft Lies : Vista sucks : How to use it a...     False  1231891200   \n",
       "7  Too bad it isn't better; also, installation pr...     False  1335398400   \n",
       "8         Beware signing up for free 3 months online     False  1167350400   \n",
       "9                                    Mutiple crashes     False  1259452800   \n",
       "\n",
       "   log_votes  isPositive  \n",
       "0   0.000000         1.0  \n",
       "1   0.000000         1.0  \n",
       "2   0.000000         1.0  \n",
       "3   0.000000         1.0  \n",
       "4   2.197225         0.0  \n",
       "5   0.000000         1.0  \n",
       "6   0.000000         0.0  \n",
       "7   0.000000         0.0  \n",
       "8   2.397895         0.0  \n",
       "9   1.098612         0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory data analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the distribution of __isPositive__ field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    34954\n",
       "0.0    21046\n",
       "Name: isPositive, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "reviewText    10\n",
      "summary       12\n",
      "verified       0\n",
      "time           0\n",
      "log_votes      0\n",
      "isPositive     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in our text fields. We will use the __reviewText__ field, so we fill-in the missing values in it with the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"].fillna(\"\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Text Processing: Stop words removal and stemming</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create the stop word removal and text cleaning processes below. NLTK library provides a list of common stop words. We will use the list, but remove some of the words from that list (because those words are actually useful to understand the sentiment in the sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get a list of stop words from the NLTK library\n",
    "stop = stopwords.words(\"english\")\n",
    "\n",
    "# These words are important for our problem. We don't want to remove them.\n",
    "excluding = [\n",
    "    \"against\",\n",
    "    \"not\",\n",
    "    \"don\",\n",
    "    \"don't\",\n",
    "    \"ain\",\n",
    "    \"aren\",\n",
    "    \"aren't\",\n",
    "    \"couldn\",\n",
    "    \"couldn't\",\n",
    "    \"didn\",\n",
    "    \"didn't\",\n",
    "    \"doesn\",\n",
    "    \"doesn't\",\n",
    "    \"hadn\",\n",
    "    \"hadn't\",\n",
    "    \"hasn\",\n",
    "    \"hasn't\",\n",
    "    \"haven\",\n",
    "    \"haven't\",\n",
    "    \"isn\",\n",
    "    \"isn't\",\n",
    "    \"mightn\",\n",
    "    \"mightn't\",\n",
    "    \"mustn\",\n",
    "    \"mustn't\",\n",
    "    \"needn\",\n",
    "    \"needn't\",\n",
    "    \"shouldn\",\n",
    "    \"shouldn't\",\n",
    "    \"wasn\",\n",
    "    \"wasn't\",\n",
    "    \"weren\",\n",
    "    \"weren't\",\n",
    "    \"won\",\n",
    "    \"won't\",\n",
    "    \"wouldn\",\n",
    "    \"wouldn't\",\n",
    "]\n",
    "\n",
    "# New stop word list\n",
    "stop_words = [word for word in stop if word not in excluding]\n",
    "\n",
    "snow = SnowballStemmer(\"english\")\n",
    "\n",
    "def process_text(texts):\n",
    "    final_text_list = []\n",
    "    for sent in texts:\n",
    "\n",
    "        # Check if the sentence is a missing value\n",
    "        if isinstance(sent, str) == False:\n",
    "            sent = \"\"\n",
    "\n",
    "        filtered_sentence = []\n",
    "        \n",
    "        # Lowercase\n",
    "        sent = sent.lower()\n",
    "        # Remove leading/trailing whitespace\n",
    "        sent = sent.strip()\n",
    "        # Remove extra space and tabs\n",
    "        sent = re.sub(\"\\s+\", \" \", sent)\n",
    "        # Remove HTML tags/markups:\n",
    "        sent = re.compile(\"<.*?>\").sub(\"\", sent)\n",
    "\n",
    "        for w in word_tokenize(sent):\n",
    "            # We are applying some custom filtering here, feel free to try different things\n",
    "            # Check if it is not numeric and its length>2 and not in stop words\n",
    "            if (not w.isnumeric()) and (len(w) > 2) and (w not in stop_words):\n",
    "                # Stem and add to filtered list\n",
    "                filtered_sentence.append(snow.stem(w))\n",
    "        final_string = \" \".join(filtered_sentence)  # final string of cleaned words\n",
    "\n",
    "        final_text_list.append(final_string)\n",
    "\n",
    "    return final_text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the effect of our preprocessing on some text from our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: \n",
      "Purchased as a quick fix for a needed Server 2003 R2 upgrade. Easy install, seamless migration. Not much MS full support left on it though. This was a one-of-a kind purchase to meet an upgrade need at a low cost. Probably a lucky find. Service and delivery were good AND the activation code worked without a burp.\n",
      "\n",
      "Preprocessed text:\n",
      "['purchas quick fix need server upgrad easi instal seamless migrat not much full support left though one-of-a kind purchas meet upgrad need low cost probabl lucki find servic deliveri good activ code work without burp']\n"
     ]
    }
   ],
   "source": [
    "review = df.reviewText.iloc[0]\n",
    "print(f\"Raw text: \\n{review}\\n\")\n",
    "print(f\"Preprocessed text:\\n{process_text([review])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text: \n",
      "I use it regularly. Great for organizing and keeping up with dates.\n",
      "\n",
      "Preprocessed text:\n",
      "['use regular great organ keep date']\n"
     ]
    }
   ],
   "source": [
    "review = df.reviewText.iloc[20]\n",
    "print(f\"Raw text: \\n{review}\\n\")\n",
    "print(f\"Preprocessed text:\\n{process_text([review])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Train - Validation Split</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's split our dataset into training (90%) and validation (10%). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    df[[\"reviewText\"]],\n",
    "    df[\"isPositive\"].values,\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the reviewText fields...\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing the reviewText fields...\")\n",
    "X_train[\"reviewText\"] = process_text(X_train[\"reviewText\"].tolist())\n",
    "X_val[\"reviewText\"] = process_text(X_val[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of features: Training and Validation\n",
      "(50400, 750) (5600, 750)\n"
     ]
    }
   ],
   "source": [
    "# Use TD-IDF to vectorize to vectors of len 750.\n",
    "tf_idf_vectorizer = TfidfVectorizer(max_features=750)\n",
    "\n",
    "# Fit the vectorizer to training data\n",
    "# Don't use the fit() on validation or test datasets\n",
    "tf_idf_vectorizer.fit(X_train[\"reviewText\"].values)\n",
    "\n",
    "# Transform text fields\n",
    "X_train = tf_idf_vectorizer.transform(X_train[\"reviewText\"].values).toarray()\n",
    "X_val = tf_idf_vectorizer.transform(X_val[\"reviewText\"].values).toarray()\n",
    "\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_train.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the tokens in the vocabulary (which we've set to be of size 750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " 'abil',\n",
       " 'abl',\n",
       " 'absolut',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'account',\n",
       " 'accur',\n",
       " 'activ',\n",
       " 'actual']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our __process_text()__ method in section 3 uses empty string for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Train the classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "## Single Layer Network/Logistic Regression \n",
    "\n",
    "To handle this binary classification problem, let's first consider a logistic regression model, mapping the input $\\mathbf{X}$ to the output $\\mathbf{y}$ by\n",
    "\n",
    "$$\n",
    "\\mathbf{\\hat{y}} = \\mathrm{sigmoid}(\\mathbf{X}\\mathbf{w} + \\mathbf{b}),\n",
    "$$\n",
    "\n",
    "with some initial choices for the parameters, $\\mathbf{w}$ weights matrix and bias $\\mathbf{b}$. We initialize the weights as random Gaussian noise, with zero mean and standard deviation 1, and start with zero bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce the single layer networks! We use the Pytorch library here. In this first part, we do the following:\n",
    "* Select our hyper-parameters to use in this problem\n",
    "* Build the network\n",
    "* Define loss function and optimizer\n",
    "* Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples to use for each weight update\n",
    "batch_size = 16\n",
    "# Total number of iterations\n",
    "# One epoch is one pass over all data in the training set\n",
    "epochs = 15\n",
    "# Learning rate\n",
    "lr = 0.01\n",
    "\n",
    "# Run the training in the GPU if supported by our instance, else in the CPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Let's build our single layer network (logistic regression here)\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features=750, # matches the size of vectorizer: 750\n",
    "              out_features=1), \n",
    "    nn.Sigmoid()\n",
    ")\n",
    "net.to(device)\n",
    "\n",
    "# Initialize the network\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, std=1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_weights)\n",
    "\n",
    "# Define the loss. For binary classification the appropriate choice is Binary Cross Entropy.\n",
    "# As we used sigmoid in the last layer, we use `nn.BCELoss`.\n",
    "# Otherwise we could have made use of `nn.BCEWithLogitsLoss`.\n",
    "loss = BCELoss(reduction=\"none\")\n",
    "\n",
    "# Define the optimizer, SGD (Stochastic Gradient Descent) with learning rate\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "# Use PyTorch DataLoaders to load the data in batches\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.float32),\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "# Move validation dataset on CPU/GPU device\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After these, we are ready to train this network. See below for the training and validation code. At high level, we have three main parts: \n",
    "\n",
    "__1. Epochs loop:__ Loop over your datast to learn and update your weights. Neural networks usually need to see the dataset multiple times to learn the better (this can be considered a slow process).\n",
    "\n",
    "__2. Training loop:__ During the training, we loop over the data loader. Data loader loads the data in batches. We selected batch size earlier. Here, the weights are updated for each batch.\n",
    "\n",
    "__3. Validation:__ After the training loop is done, we go over the validation data and calculate the validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.6074669294508677, Validation_loss 0.5327882603236607, Seconds 1.9980592727661133\n",
      "Epoch 1. Train_loss 0.4913513486820554, Validation_loss 0.4718841552734375, Seconds 2.05788254737854\n",
      "Epoch 2. Train_loss 0.448418990118163, Validation_loss 0.44151929582868304, Seconds 1.977966070175171\n",
      "Epoch 3. Train_loss 0.4245563501025003, Validation_loss 0.4227579171316964, Seconds 1.9499552249908447\n",
      "Epoch 4. Train_loss 0.4089667638403083, Validation_loss 0.4099011666434152, Seconds 2.079286813735962\n",
      "Epoch 5. Train_loss 0.39789008383003494, Validation_loss 0.4005377197265625, Seconds 2.0015697479248047\n",
      "Epoch 6. Train_loss 0.3895980614163573, Validation_loss 0.3934344482421875, Seconds 2.0712027549743652\n",
      "Epoch 7. Train_loss 0.38316069374718364, Validation_loss 0.3878836495535714, Seconds 2.20481014251709\n",
      "Epoch 8. Train_loss 0.3780253144933118, Validation_loss 0.3834462629045759, Seconds 1.9737942218780518\n",
      "Epoch 9. Train_loss 0.37384031818263114, Validation_loss 0.37983407156808036, Seconds 1.7915241718292236\n",
      "Epoch 10. Train_loss 0.3703705281776095, Validation_loss 0.3768501935686384, Seconds 2.070319175720215\n",
      "Epoch 11. Train_loss 0.3674524226808359, Validation_loss 0.37435498918805804, Seconds 2.0402884483337402\n",
      "Epoch 12. Train_loss 0.36496867818018747, Validation_loss 0.3722468784877232, Seconds 2.1441116333007812\n",
      "Epoch 13. Train_loss 0.3628328021296433, Validation_loss 0.3704503086635045, Seconds 1.9176764488220215\n",
      "Epoch 14. Train_loss 0.36097981401615675, Validation_loss 0.3689076450892857, Seconds 1.8945868015289307\n"
     ]
    }
   ],
   "source": [
    "# Lists to store the losses as the training progresses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    # Build a training loop to train the network\n",
    "    for data, target in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device).view(-1, 1)\n",
    "\n",
    "        # Forward pass - compute the predictions of the NN on the batch\n",
    "        output = net(data)  \n",
    "        # Compute the loss and sum (error between the net's predictions and the actual labels)\n",
    "        L = loss(output, target).sum()\n",
    "        training_loss += L.item() \n",
    "        # Calculate gradients\n",
    "        L.backward()  \n",
    "        # Update weights with gradient descent\n",
    "        optimizer.step()  \n",
    "\n",
    "    # Get validation predictions\n",
    "    val_predictions = net(X_val)\n",
    "    # Calculate the validation loss\n",
    "    val_loss = torch.sum(loss(val_predictions, y_val.view(-1, 1))).item()\n",
    "\n",
    "    # Take the average losses\n",
    "    training_loss = training_loss / len(y_train)\n",
    "    val_loss = val_loss / len(y_val)\n",
    "\n",
    "    train_losses.append(training_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss}, Validation_loss {val_loss}, Seconds {end-start}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label=\"Training Loss\")\n",
    "plt.plot(val_losses, label=\"Validation Loss\")\n",
    "plt.title(\"Loss values\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Make predictions on the validation data and test the classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's get the validation predictions with this ```val_predictions = net(X_val)```. Then, we look at our validation data performance. See comments in the code for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions using the trained network: \"net\"\n",
    "val_predictions = net(X_val)\n",
    "# Round up (to 1) or down (to 0) the result (remember the sigmoid).\n",
    "# Use np.rint() for that\n",
    "val_predictions = np.rint(val_predictions.detach().cpu().numpy())\n",
    "print(val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of the one extra level in this array. We will simply use the np.squeeze() function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "val_predictions = np.squeeze(val_predictions)\n",
    "print(val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix, classification report and accuracy score are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1504  548]\n",
      " [ 401 3147]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.73      0.76      2052\n",
      "         1.0       0.85      0.89      0.87      3548\n",
      "\n",
      "    accuracy                           0.83      5600\n",
      "   macro avg       0.82      0.81      0.81      5600\n",
      "weighted avg       0.83      0.83      0.83      5600\n",
      "\n",
      "Accuracy (validation): 0.8305357142857143\n"
     ]
    }
   ],
   "source": [
    "y_val = y_val.detach().cpu().numpy()\n",
    "print(confusion_matrix(y_val, val_predictions))\n",
    "print(classification_report(y_val, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(y_val, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. <a name=\"8\">Getting predictions on the test data and saving results</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = process_text(df_test[\"reviewText\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of features: Training and Validation\n",
      "(14000, 750)\n"
     ]
    }
   ],
   "source": [
    "X_test = tf_idf_vectorizer.transform(X_test).toarray()\n",
    "\n",
    "print(\"Shapes of features: Training and Validation\")\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14000, 750])\n"
     ]
    }
   ],
   "source": [
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions using the trained network: \"net\"\n",
    "test_predictions = net(X_test)\n",
    "# Round up (to 1) or down (to 0) the result (remember the sigmoid).\n",
    "# Use np.rint() for that\n",
    "test_predictions = np.rint(test_predictions.detach().cpu().numpy())\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = np.squeeze(test_predictions)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = df_test[\"ID\"]\n",
    "result_df[\"isPositive\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"result_day1_logistic_regr.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. <a name=\"9\">Ideas for improvement</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can usually improve performance with some additional work. You can try the following:\n",
    "* Experiment with different values for the hyperparameters: Batch size and learning rate.\n",
    "* How about adding another layer? It is tomorrow's topic, but you can try adding another layer. Check out the MLA-NLP-DAY2-NN-NB notebook from tomorrow's content if you are interested.\n",
    "* Come up with some other features such as having certain punctuations, all-capitalized words or some words that might be useful in this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
