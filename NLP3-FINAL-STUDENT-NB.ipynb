{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Day 3: Use LSTM or fine-tune BERT for the Product Safety Dataset\n",
    "\n",
    "We continue to work with the final project dataset. This time you can work with [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) (its use is similar to RNN) or fine-tune a BERT model. Be careful with the BERT approach as it takes a long time. You will again predict the __human_tag__ field of the dataset.\n",
    "\n",
    "Use the notebooks from the class and implement the model, train and test with the corresponding datasets.\n",
    "You can follow these steps:\n",
    "1. Read training-test data (Given)\n",
    "2. Train a classifier (Implement)\n",
    "3. Make predictions on your test dataset (Implement)\n",
    "4. Write your test predictions to a CSV file (Given)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade dependencies\n",
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import re, time\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import torch, torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchtext.vocab import GloVe\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Reading the dataset\n",
    "\n",
    "We will use the __pandas__ library to read our dataset. Let's first download the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Training data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "      <th>human_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47490</td>\n",
       "      <td>15808037321</td>\n",
       "      <td>I ordered a sample of the Dietspotlight Burn, ...</td>\n",
       "      <td>6/25/2018 17:51</td>\n",
       "      <td>1</td>\n",
       "      <td>DO NOT BUY!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16127</td>\n",
       "      <td>16042300811</td>\n",
       "      <td>This coffee tasts terrible as if it got burnt ...</td>\n",
       "      <td>2/8/2018 15:59</td>\n",
       "      <td>2</td>\n",
       "      <td>Coffee not good</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51499</td>\n",
       "      <td>16246716471</td>\n",
       "      <td>I've been buying lightly salted Planters cashe...</td>\n",
       "      <td>3/22/2018 17:53</td>\n",
       "      <td>2</td>\n",
       "      <td>Poor Quality - Burnt, Shriveled Nuts With Blac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>36725</td>\n",
       "      <td>14460351031</td>\n",
       "      <td>This product is great in so many ways. It goes...</td>\n",
       "      <td>12/7/2017 8:49</td>\n",
       "      <td>4</td>\n",
       "      <td>Very lovey product, good sunscreen, but strong...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>49041</td>\n",
       "      <td>15509997211</td>\n",
       "      <td>My skin did not agree with this product, it wo...</td>\n",
       "      <td>3/21/2018 13:51</td>\n",
       "      <td>1</td>\n",
       "      <td>Not for everyone. Reactions can be harsh.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  47490  15808037321  I ordered a sample of the Dietspotlight Burn, ...   \n",
       "1  16127  16042300811  This coffee tasts terrible as if it got burnt ...   \n",
       "2  51499  16246716471  I've been buying lightly salted Planters cashe...   \n",
       "3  36725  14460351031  This product is great in so many ways. It goes...   \n",
       "4  49041  15509997211  My skin did not agree with this product, it wo...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0  6/25/2018 17:51            1   \n",
       "1   2/8/2018 15:59            2   \n",
       "2  3/22/2018 17:53            2   \n",
       "3   12/7/2017 8:49            4   \n",
       "4  3/21/2018 13:51            1   \n",
       "\n",
       "                                               title  human_tag  \n",
       "0                                        DO NOT BUY!          0  \n",
       "1                                    Coffee not good          0  \n",
       "2  Poor Quality - Burnt, Shriveled Nuts With Blac...          0  \n",
       "3  Very lovey product, good sunscreen, but strong...          0  \n",
       "4          Not for everyone. Reactions can be harsh.          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('../../data/final_project/training.csv', encoding='utf-8', header=0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### __Test data:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62199</td>\n",
       "      <td>15449606311</td>\n",
       "      <td>Quality of material is great, however, the bac...</td>\n",
       "      <td>3/7/2018 19:47</td>\n",
       "      <td>3</td>\n",
       "      <td>great backpack with strange fit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76123</td>\n",
       "      <td>15307152511</td>\n",
       "      <td>The product was okay but wasn't refined campho...</td>\n",
       "      <td>43135.875</td>\n",
       "      <td>2</td>\n",
       "      <td>Not refined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78742</td>\n",
       "      <td>12762748321</td>\n",
       "      <td>I normally read the reviews before buying some...</td>\n",
       "      <td>42997.37708</td>\n",
       "      <td>1</td>\n",
       "      <td>Doesnt work, wouldnt recommend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64010</td>\n",
       "      <td>15936405041</td>\n",
       "      <td>These pads are completely worthless. The light...</td>\n",
       "      <td>43313.25417</td>\n",
       "      <td>1</td>\n",
       "      <td>The lighter colored side of the pads smells li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17058</td>\n",
       "      <td>13596875291</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "      <td>12/5/2017 20:17</td>\n",
       "      <td>2</td>\n",
       "      <td>The saw works great but the blade oiler does n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID       doc_id                                               text  \\\n",
       "0  62199  15449606311  Quality of material is great, however, the bac...   \n",
       "1  76123  15307152511  The product was okay but wasn't refined campho...   \n",
       "2  78742  12762748321  I normally read the reviews before buying some...   \n",
       "3  64010  15936405041  These pads are completely worthless. The light...   \n",
       "4  17058  13596875291  The saw works great but the blade oiler does n...   \n",
       "\n",
       "              date  star_rating  \\\n",
       "0   3/7/2018 19:47            3   \n",
       "1        43135.875            2   \n",
       "2      42997.37708            1   \n",
       "3      43313.25417            1   \n",
       "4  12/5/2017 20:17            2   \n",
       "\n",
       "                                               title  \n",
       "0                    great backpack with strange fit  \n",
       "1                                        Not refined  \n",
       "2                     Doesnt work, wouldnt recommend  \n",
       "3  The lighter colored side of the pads smells li...  \n",
       "4  The saw works great but the blade oiler does n...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../../data/final_project/test.csv', encoding='utf-8', header=0)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    53375\n",
       "1     9759\n",
       "Name: human_tag, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"human_tag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "doc_id         0\n",
      "text           6\n",
      "date           0\n",
      "star_rating    0\n",
      "title          1\n",
      "human_tag      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['text'] = train_df['text'].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this\n",
    "# Implement this\n",
    "train_text, val_text, train_label, val_label = train_test_split(\n",
    "    train_df[\"text\"].tolist(),\n",
    "    train_df[\"human_tag\"].tolist(),\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', 'the', 'i', ',', 'it', 'and', 'to', 'a', \"'\", 'of', 'this', 'is', 'my', 'for', 'in', 'that', 'not', 'on', 'but', 'was', 'you', 't', 'with']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "counter = Counter()\n",
    "for line in train_text:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=2) #min_freq>1 for skipping misspelled words\n",
    "\n",
    "print(vocab.itos[0:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a mapper to transform our text data\n",
    "text_transform_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform:\tHorrible. Other reviewers have said it worked on their sensitive skin - this burned intensely immediately when I put a tiny drop on just to test it out. I washed it out thoroughly immediately but still got a rash. I wish I could return this!\n",
      "After transform:\t[408, 2, 90, 1356, 25, 278, 6, 197, 19, 185, 359, 86, 100, 12, 54, 6745, 411, 40, 4, 123, 9, 715, 1050, 19, 46, 8, 554, 6, 38, 2, 4, 762, 6, 38, 2229, 411, 20, 106, 99, 9, 1034, 2, 4, 338, 4, 114, 205, 12, 26]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before transform:\\t{train_text[37]}\")\n",
    "print(f\"After transform:\\t{text_transform_pipeline(train_text[37])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_split, seq_length):\n",
    "    # Transform the text\n",
    "    # use the dict to tokenize each review in reviews_split\n",
    "    # store the tokenized reviews in reviews_ints\n",
    "    reviews_ints = []\n",
    "    for review in reviews_split:\n",
    "        reviews_ints.append(text_transform_pipeline(review))\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.ones((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    # for each review, I grab that review\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: I was looking for an electric smoker in hopes that I could smoke during the winter, when temperatures were in the single digits. Mainly for maintaining the temperatures during smoking and cooking. Well, it does, it worked great in single digit temperatures. I was also glad that I chose this model because of the insulated door and walls. The units with glass doors were questionable to me regarding maintenance of temperatures. The only down side I have found with this unit and the reason for not giving it a five star is this. I found that with ambient temperatures in the 80s or higher the units heater doesn't turn on enough to light the wood chips and create smoke. What I have had to do is half latch the door, so the door is slightly open and allows the heat out and the heating elements will remain on to try and raise the internal temp to what you have set, causing the heating elements to heat up enough to get the wood chips burning, once they are lit I close the door. I am very happy with the unit it has worked flawlessly for me.\n",
      "\n",
      "Original length of the text: 1044\n",
      "\n",
      "Transformed text: \n",
      "tensor([[    4,    21,   269,    15,    72,   901,  1546,    16,  1545,    17,\n",
      "             4,   114,   372,   459,     3,  1431,     5,    40,  2041,   101,\n",
      "            16,     3,   720, 11197,     2,  2364,    15,  5651,     3,  2041,\n",
      "           459,   731,     7,   356,     2,    77,     5,     6,    95,     5,\n",
      "             6,   197,    92,    16,   720, 14479,  2041,     2,     4,    21]])\n",
      "\n",
      "Shape of transformed text: torch.Size([1, 50])\n",
      "\n",
      "Text: I really want to like these earphones, but theres just too many small things that make them unlikable.<br />- Before you even get house them they need to go through a 72 burn in period.  Whaaaat?  I understand what burn in is.  I don't understand why earphones need during in.  Even less, if it's required, why can't it be one at the factory.<br />- The volume is low.  As in, barely adequate.  Compared to my Etymotic 3.6mm Headphones, it's downright paltry.<br />- The noise reduction is passable, but not great.  I understand that they're earphones, and that likely has a lot to do with it.<br />- The microphone is simply horrible.  I have to put the mic right up to my mouth in order for anyone to hear me, and that includes in an otherwise quiet room.<br />- The control module is awkward to operate and somewhat heavy.  The weight is especially noticeable if, like me, you have large ear canals, and the phones tend to come out easily.  Even with aftermarket replacement earphones.<br /><br />Overall, they sound nice, but physically they are a disappointment.\n",
      "\n",
      "Original length of the text: 1067\n",
      "\n",
      "Transformed text: \n",
      "tensor([[    4,    80,   171,     8,    31,    59,  5216,     5,    20,  4198,\n",
      "            46,    91,   240,   156,   295,    17,   135,    60,     0,     2,\n",
      "           100,   120,    22,    78,    55,   300,    60,    29,   159,     8,\n",
      "           169,   139,     9,  6175,    34,    16,  1118,     2, 30882,   124,\n",
      "             4,   968,    84,    34,    16,    13,     2,     4,    73,    10]])\n",
      "\n",
      "Shape of transformed text: torch.Size([1, 50])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in train_text[9:11]:\n",
    "    print(f\"Text: {text}\\n\")\n",
    "    print(f\"Original length of the text: {len(text)}\\n\")\n",
    "    tt = pad_features([text], seq_length=50)\n",
    "    print(f\"Transformed text: \\n{tt}\\n\")\n",
    "    print(f\"Shape of transformed text: {tt.shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Pass transformed and padded data to dataset\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    pad_features(train_text, max_len), torch.tensor(train_label, dtype=torch.float32)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(pad_features(val_text, max_len), torch.tensor(val_label, dtype=torch.float32))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = GloVe(name=\"6B\", dim=300)\n",
    "embedding_matrix = glove.get_vecs_by_tokens(vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 128\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "embed_size = 300  # glove.6B.300d.txt\n",
    "vocab_size = len(vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=1)\n",
    "        self.lstm = nn.LSTM(\n",
    "            embed_size, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, 1)  # <==============\n",
    "        #self.linear = nn.Linear(hidden_size, 5)  # <==============\n",
    "        self.act = nn.Sigmoid()\n",
    "        #self.act = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # Call the RNN layer\n",
    "        outputs, _ = self.lstm(embeddings)\n",
    "        \n",
    "        # Output shape after RNN: (batch_size, max_len, hidden_size)\n",
    "        # Get the output from the last time step with outputs[:, -1, :] below\n",
    "        # The output shape becomes: (batch_size, 1, hidden_size)\n",
    "        # Send it to the linear layer\n",
    "        outs = self.linear(outputs[:, -1, :])\n",
    "        return self.act(outs)\n",
    "    \n",
    "# Initialize the weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.LSTM:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our architecture with 2 RNN layers\n",
    "model = Net(vocab_size, embed_size, hidden_size, num_layers=2)\n",
    "\n",
    "# We set the embedding layer's parameters from GloVe\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "# We won't change/train the embedding layer\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (embedding): Embedding(31128, 300, padding_idx=1)\n",
       "  (lstm): LSTM(300, 128, num_layers=2, batch_first=True)\n",
       "  (linear): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting our trainer\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We will use Binary Cross-entropy loss\n",
    "# reduction=\"sum\" sums the losses for given output and target\n",
    "cross_ent_loss = nn.BCELoss(reduction=\"sum\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n",
      "Epoch 0. Train_loss 0.4572097093024249. Val_loss 0.4321973819992457. Seconds 11.199868440628052\n",
      "Epoch 1. Train_loss 0.430823131582763. Val_loss 0.4315806000074858. Seconds 11.2220299243927\n",
      "Epoch 2. Train_loss 0.43015274879001725. Val_loss 0.43093157071345284. Seconds 11.332505226135254\n",
      "Epoch 3. Train_loss 0.4294619638959273. Val_loss 0.43025316254111606. Seconds 11.291494607925415\n",
      "Epoch 4. Train_loss 0.4287265259126088. Val_loss 0.4295192491339852. Seconds 11.277247190475464\n",
      "Epoch 5. Train_loss 0.42791628057123365. Val_loss 0.4286992879921324. Seconds 11.038425207138062\n",
      "Epoch 6. Train_loss 0.4269944144486961. Val_loss 0.42775529572055204. Seconds 11.061888933181763\n",
      "Epoch 7. Train_loss 0.42591312674307896. Val_loss 0.4266372938340398. Seconds 10.995811462402344\n",
      "Epoch 8. Train_loss 0.42460711478179963. Val_loss 0.4252764335115159. Seconds 10.991759538650513\n",
      "Epoch 9. Train_loss 0.4229831214355608. Val_loss 0.42357344804098607. Seconds 11.07912015914917\n",
      "Epoch 10. Train_loss 0.4209018132922432. Val_loss 0.42137799762946954. Seconds 11.052520036697388\n",
      "Epoch 11. Train_loss 0.4181437364307687. Val_loss 0.418447695542711. Seconds 11.046955108642578\n",
      "Epoch 12. Train_loss 0.41433982006557724. Val_loss 0.41436047111465313. Seconds 10.972224235534668\n",
      "Epoch 13. Train_loss 0.4088266692082675. Val_loss 0.40831391072024037. Seconds 11.087652683258057\n",
      "Epoch 14. Train_loss 0.4004246995718451. Val_loss 0.3986739150172062. Seconds 11.06236457824707\n",
      "Epoch 15. Train_loss 0.3877488934376927. Val_loss 0.38322072033645005. Seconds 11.034019708633423\n",
      "Epoch 16. Train_loss 0.3718609217395668. Val_loss 0.3637181930916574. Seconds 11.812939882278442\n",
      "Epoch 17. Train_loss 0.3591120661333051. Val_loss 0.35002817683179127. Seconds 11.100131034851074\n",
      "Epoch 18. Train_loss 0.35322453305151463. Val_loss 0.3445666658753717. Seconds 11.094145774841309\n",
      "Epoch 19. Train_loss 0.3497391589801524. Val_loss 0.3410344225037955. Seconds 11.00097370147705\n"
     ]
    }
   ],
   "source": [
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device = \", device)\n",
    "\n",
    "model.apply(init_weights)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    val_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for data, target in train_loader:\n",
    "        trainer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        L = cross_ent_loss(output, target.unsqueeze(1))\n",
    "        training_loss += L.item()\n",
    "        L.backward()\n",
    "        trainer.step()\n",
    "\n",
    "    # Validate the network, no training (no weight update)\n",
    "    for data, target in val_loader:\n",
    "        val_predictions = model(data.to(device))\n",
    "        L = cross_ent_loss(val_predictions, target.to(device).unsqueeze(1))\n",
    "        val_loss += L.item()\n",
    "\n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss / len(val_label)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss}. Val_loss {val_loss}. Seconds {end-start}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "val_predictions = []\n",
    "for data, target in val_loader:\n",
    "    val_preds = model(data.to(device))\n",
    "    val_predictions.extend(\n",
    "        [np.rint(val_pred)[0] for val_pred in val_preds.detach().cpu().numpy()]\n",
    "    )\n",
    "print(val_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5223  110]\n",
      " [ 795  186]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.98      0.92      5333\n",
      "           1       0.63      0.19      0.29       981\n",
      "\n",
      "    accuracy                           0.86      6314\n",
      "   macro avg       0.75      0.58      0.61      6314\n",
      "weighted avg       0.83      0.86      0.82      6314\n",
      "\n",
      "Accuracy (validation): 0.856667722521381\n"
     ]
    }
   ],
   "source": [
    "# Use the fitted pipeline to make predictions on the validation dataset\n",
    "print(confusion_matrix(val_label, val_predictions))\n",
    "print(classification_report(val_label, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(val_label, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Make predictions on your test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement this\n",
    "test_text = test_df[\"text\"].fillna(value=\"missing\").tolist()\n",
    "\n",
    "test_dataset = TensorDataset(pad_features(test_text, max_len)) #, torch.tensor(val_label))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "for data, in test_loader:\n",
    "    test_preds = model(data.to(device))\n",
    "    test_predictions.extend(\n",
    "        [np.rint(test_pred)[0] for test_pred in test_preds.detach().cpu().numpy()]\n",
    "    )\n",
    "print(test_predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write your predictions to a CSV file\n",
    "You can use the following code to write your test predictions to a CSV file. Then upload your file to https://mlu.corp.amazon.com/contests/redirect/53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = test_df[\"ID\"]\n",
    "result_df[\"human_tag\"] = test_predictions\n",
    " \n",
    "#result_df.to_csv(\"../../data/final_project/project_day3_result.csv\", encoding='utf-8', index=False)\n",
    "result_df.to_csv(\"./project_day3_result.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
