{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"0\">Machine Learning Accelerator - Natural Language Processing - Lecture 2</a>\n",
    "\n",
    "## Recurrent Neural Networks (RNNs) for the Product Review Problem - Classify Product Reviews as Positive or Not\n",
    "\n",
    "In this exercise, we will learn how to use Recurrent Neural Networks. \n",
    "\n",
    "We will follow these steps:\n",
    "1. <a href=\"#1\">Reading the dataset</a>\n",
    "2. <a href=\"#2\">Exploratory data analysis</a>\n",
    "3. <a href=\"#3\">Train-validation dataset split</a>\n",
    "4. <a href=\"#4\">Text Transformation</a>\n",
    "5. <a href=\"#5\">Generating data batch and iterator</a>\n",
    "6. <a href=\"#6\">Using pre-trained GloVe Word Embeddings</a>\n",
    "7. <a href=\"#7\">Setting Hyperparameters and Bulding the Network</a>\n",
    "8. <a href=\"#8\">Training the Network</a>\n",
    "9. <a href=\"#9\">Improvement ideas</a>\n",
    "\n",
    "Overall dataset schema:\n",
    "* __reviewText:__ Text of the review\n",
    "* __summary:__ Summary of the review\n",
    "* __verified:__ Whether the purchase was verified (True or False)\n",
    "* __time:__ UNIX timestamp for the review\n",
    "* __log_votes:__ Logarithm-adjusted votes log(1+votes)\n",
    "* __isPositive:__ Whether the review is positive or negative (1 or 0)\n",
    "\n",
    "__Important note:__ One big distinction betweeen the regular neural networks and RNNs is that RNNs work with sequential data. In our case, RNNs will help us with the text field. If we also want to consider other fields such as time, log_votes, verified, etc., we need to use the regular neural networks with the RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:47.162268Z",
     "start_time": "2021-01-09T05:02:47.160085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==1.8.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 1)) (1.8.1+cu101)\n",
      "Requirement already satisfied: torchtext==0.9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 2)) (0.9.1)\n",
      "Requirement already satisfied: nltk==3.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 3)) (3.6.2)\n",
      "Requirement already satisfied: pandas==1.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 4)) (1.1.5)\n",
      "Requirement already satisfied: scikit-learn==0.24.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 5)) (0.24.1)\n",
      "Requirement already satisfied: numpy==1.19.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 6)) (1.19.5)\n",
      "Requirement already satisfied: trax==1.3.7 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 7)) (1.3.7)\n",
      "Requirement already satisfied: transformers==4.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from -r ../../requirements.txt (line 8)) (4.5.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 1)) (0.8)\n",
      "Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch==1.8.1->-r ../../requirements.txt (line 1)) (3.7.4.3)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 2)) (4.61.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2.26.0)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (8.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk==3.6.2->-r ../../requirements.txt (line 3)) (2021.4.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 4)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas==1.1.5->-r ../../requirements.txt (line 4)) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 5)) (2.1.0)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from scikit-learn==0.24.1->-r ../../requirements.txt (line 5)) (1.5.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (1.15.0)\n",
      "Requirement already satisfied: jax in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.2.17)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (5.8.0)\n",
      "Requirement already satisfied: absl-py in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: gym in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.21.0)\n",
      "Requirement already satisfied: t5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.9.3)\n",
      "Requirement already satisfied: funcsigs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (1.0.2)\n",
      "Requirement already satisfied: jaxlib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.1.69)\n",
      "Requirement already satisfied: tensorflow-datasets in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (4.4.0)\n",
      "Requirement already satisfied: tensorflow-text in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: gin-config in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from trax==1.3.7->-r ../../requirements.txt (line 7)) (0.5.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (4.8.1)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (0.10.3)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (21.0)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (0.0.46)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.0.12)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from gym->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: opt-einsum in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jax->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from jaxlib->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.12)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.5.1->-r ../../requirements.txt (line 8)) (3.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->torchtext==0.9.1->-r ../../requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: sacrebleu in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.0.0)\n",
      "Requirement already satisfied: seqio in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.0.7)\n",
      "Requirement already satisfied: tfds-nightly in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (4.4.0.dev202112080110)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.1.96)\n",
      "Requirement already satisfied: rouge-score in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.0.4)\n",
      "Requirement already satisfied: mesh-tensorflow[transformer]>=0.1.13 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.1.19)\n",
      "Requirement already satisfied: editdistance in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.6.0)\n",
      "Requirement already satisfied: babel in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.9.1)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.19.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.2.0)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.3.4)\n",
      "Requirement already satisfied: importlib-resources in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (5.4.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (21.2.0)\n",
      "Requirement already satisfied: promise in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.3)\n",
      "Requirement already satisfied: termcolor in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.1.0)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.18.2)\n",
      "Requirement already satisfied: tensorflow<2.7,>=2.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.6.2)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.12.0)\n",
      "Requirement already satisfied: google-pasta~=0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.12.1)\n",
      "Requirement already satisfied: tensorboard<2.7,>=2.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.1.2)\n",
      "Requirement already satisfied: gast==0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.4.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.6.3)\n",
      "Requirement already satisfied: h5py~=3.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.1.0)\n",
      "Requirement already satisfied: keras<2.7,>=2.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.36.2)\n",
      "Requirement already satisfied: clang~=5.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (5.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.42.0)\n",
      "Requirement already satisfied: colorama in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.4.3)\n",
      "Requirement already satisfied: portalocker in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.3.2)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacrebleu->t5->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.8.9)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorflow-metadata->tensorflow-datasets->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.53.0)\n",
      "Requirement already satisfied: cached-property in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from h5py~=3.1.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.5.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.4.6)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.30.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (58.3.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.8.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow<2.7,>=2.6.0->tensorflow-text->trax==1.3.7->-r ../../requirements.txt (line 7)) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "# Upgrade dependencies\n",
    "!pip install -r ../../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:48.342987Z",
     "start_time": "2021-01-09T05:02:47.164823Z"
    }
   },
   "outputs": [],
   "source": [
    "import re, time\n",
    "import numpy as np\n",
    "import torch, torchtext\n",
    "import boto3\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from os import path\n",
    "from collections import Counter\n",
    "from torch import nn, optim\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a name=\"1\">Reading the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset below and look at the first five rows in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:48.995226Z",
     "start_time": "2021-01-09T05:02:48.344888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>verified</th>\n",
       "      <th>time</th>\n",
       "      <th>log_votes</th>\n",
       "      <th>isPositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65886</td>\n",
       "      <td>Purchased as a quick fix for a needed Server 2...</td>\n",
       "      <td>Easy install, seamless migration</td>\n",
       "      <td>True</td>\n",
       "      <td>1458864000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19822</td>\n",
       "      <td>So far so good. Installation was simple. And r...</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>True</td>\n",
       "      <td>1417478400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14558</td>\n",
       "      <td>Microsoft keeps making Visual Studio better. I...</td>\n",
       "      <td>This is the best development tool I've ever used.</td>\n",
       "      <td>False</td>\n",
       "      <td>1252886400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39708</td>\n",
       "      <td>Very good product.</td>\n",
       "      <td>Very good product.</td>\n",
       "      <td>True</td>\n",
       "      <td>1458604800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8015</td>\n",
       "      <td>So very different from my last version and I a...</td>\n",
       "      <td>... from my last version and I am having a gre...</td>\n",
       "      <td>True</td>\n",
       "      <td>1454716800</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                         reviewText  \\\n",
       "0  65886  Purchased as a quick fix for a needed Server 2...   \n",
       "1  19822  So far so good. Installation was simple. And r...   \n",
       "2  14558  Microsoft keeps making Visual Studio better. I...   \n",
       "3  39708                                 Very good product.   \n",
       "4   8015  So very different from my last version and I a...   \n",
       "\n",
       "                                             summary  verified        time  \\\n",
       "0                   Easy install, seamless migration      True  1458864000   \n",
       "1                                         Five Stars      True  1417478400   \n",
       "2  This is the best development tool I've ever used.     False  1252886400   \n",
       "3                                 Very good product.      True  1458604800   \n",
       "4  ... from my last version and I am having a gre...      True  1454716800   \n",
       "\n",
       "   log_votes  isPositive  \n",
       "0   0.000000         1.0  \n",
       "1   0.000000         1.0  \n",
       "2   0.000000         1.0  \n",
       "3   0.000000         1.0  \n",
       "4   2.197225         0.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TRAINING.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory data analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the range and distribution of the target column `isPositive`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.024615Z",
     "start_time": "2021-01-09T05:02:49.017492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0    34954\n",
       "0.0    21046\n",
       "Name: isPositive, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"isPositive\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the number of missing values for each columm below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.040120Z",
     "start_time": "2021-01-09T05:02:49.026288Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID             0\n",
      "reviewText    10\n",
      "summary       12\n",
      "verified       0\n",
      "time           0\n",
      "log_votes      0\n",
      "isPositive     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have missing values in our text fields. We will use the __reviewText__ field, so we fill-in the missing values in it iwth the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"reviewText\"] = df[\"reviewText\"].fillna(\"missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Train-validation split</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:02:49.098503Z",
     "start_time": "2021-01-09T05:02:49.041948Z"
    }
   },
   "outputs": [],
   "source": [
    "# This separates 10% of the entire dataset into validation dataset.\n",
    "train_text, val_text, train_label, val_label = train_test_split(\n",
    "    df[\"reviewText\"].tolist(),\n",
    "    df[\"isPositive\"].tolist(),\n",
    "    test_size=0.10,\n",
    "    shuffle=True,\n",
    "    random_state=324,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Text transformation</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will apply the following processes here:\n",
    "1. Creating a vocabulary\n",
    "2. Text transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Creating a vocabulary:__ \n",
    "\n",
    "We will create a vocabulary with the tokens from the text data. We use a simple english tokenizer and use these tokens to create our vocabulary. In this vocabulary, tokens will map to unique ids, such as \"car\"->32, \"house\"->651, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "counter = Counter()\n",
    "for line in train_text:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=2) #min_freq>1 for skipping misspelled words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'home' -> 211\n",
      "'wash' -> 10241\n",
      "'fhshbasdhb' -> 0\n"
     ]
    }
   ],
   "source": [
    "print(f\"'home' -> {vocab['home']}\")\n",
    "print(f\"'wash' -> {vocab['wash']}\")\n",
    "# unknown word (assume from test set)\n",
    "print(f\"'fhshbasdhb' -> {vocab['fhshbasdhb']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the words for the first 25 indexes in the vocabulary. '< unk >' is reserved for unknown words '< pad >' is used for the padded tokens (we will learn in a minute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', '.', 'the', 'i', ',', 'to', 'and', 'it', 'a', \"'\", 'of', 'is', 'for', 'this', 'you', 'that', 'my', 'with', 'in', 'have', 'on', 'not', 'was', 'but']\n"
     ]
    }
   ],
   "source": [
    "print(vocab.itos[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2. Text transformation:__ \n",
    "\n",
    "We will use the vocabulary and map tokens in the text to unique ids of the tokens. For example: `[\"this\", \"is\", \"a\", \"sentence\"] -> [14, 12, 9, 2066]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a mapper to transform our text data\n",
    "text_transform_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see some text before and after transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform:\tHappy to own it.\n",
      "After transform:\t[321, 6, 237, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before transform:\\t{train_text[37]}\")\n",
    "print(f\"After transform:\\t{text_transform_pipeline(train_text[37])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function for this. In this function, we transform and pad (if necessary) our text data. We cut the series of words at the point where it reaches a certain lenght (we used `max_len=50` here). If the text is shorter than max_len, we `pad ones` to the start of the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_split, seq_length):\n",
    "    # Transform the text\n",
    "    # use the dict to tokenize each review in reviews_split\n",
    "    # store the tokenized reviews in reviews_ints\n",
    "    reviews_ints = []\n",
    "    for review in reviews_split:\n",
    "        reviews_ints.append(text_transform_pipeline(review))\n",
    "    \n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.ones((len(reviews_ints), seq_length), dtype=int)\n",
    "    \n",
    "    # for each review, I grab that review\n",
    "    for i, row in enumerate(reviews_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return torch.tensor(features, dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see two example sentences. Remember that 1 is used for each padded item and 0 is used for each unknown word (if there is any in the given text)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Didnt give a 5 because I don't know what I need. I like it great\n",
      "\n",
      "Original length of the text: 64\n",
      "\n",
      "Transformed text: \n",
      "tensor([[   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1, 4934,  240,    9,  189,\n",
      "          100,    4,   85,   10,   25,  155,   67,    4,  109,    2,    4,   60,\n",
      "            8,   68]])\n",
      "\n",
      "Shape of transformed text: torch.Size([1, 50])\n",
      "\n",
      "Text: I downloaded the 30 day trial onto my wife's Mac about a week ago, but I don't know if I did it correctly as she is still getting spam..but not quite as much. Perhaps someone has information on this as I don't find a way to send an email to the company to see if I am in the trial period.\n",
      "\n",
      "Original length of the text: 288\n",
      "\n",
      "Transformed text: \n",
      "tensor([[   4,  347,    3,  695,  307,  559,  895,   17,  990,   10,   29,  132,\n",
      "           83,    9,  598,  354,    5,   24,    4,   85,   10,   25,  155,   36,\n",
      "            4,  104,    8,  758,   27,  387,   12,  116,  257, 1721,    2,    2,\n",
      "           24,   22,  386,   27,   99,    2,  980,  456,   58,  263,   21,   14,\n",
      "           27,    4]])\n",
      "\n",
      "Shape of transformed text: torch.Size([1, 50])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for text in train_text[9:11]:\n",
    "    print(f\"Text: {text}\\n\")\n",
    "    print(f\"Original length of the text: {len(text)}\\n\")\n",
    "    tt = pad_features([text], seq_length=50)\n",
    "    print(f\"Transformed text: \\n{tt}\\n\")\n",
    "    print(f\"Shape of transformed text: {tt.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Generating data batch and iterator</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's use the pad_features() function and create the data loaders. Here, we use __max_len=50__ to consider the first 50 words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "batch_size = 64\n",
    "\n",
    "# Pass transformed and padded data to dataset\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(\n",
    "    pad_features(train_text, max_len), torch.tensor(train_label, dtype=torch.float32)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "\n",
    "val_dataset = TensorDataset(pad_features(val_text, max_len), torch.tensor(val_label, dtype=torch.float32))\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Using pre-trained GloVe word embeddings</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In this example, we will use GloVe word vectors. `name='6B'` `dim=300` gives us 6 billion words/phrases vectors. Each word vector has 300 numbers in it. The following code shows how to get the word vectors and create an embedding matrix from them. We will connect our vocabulary indexes to the GloVe embedding with the `get_vecs_by_tokens()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.864398Z",
     "start_time": "2021-01-09T05:04:29.376025Z"
    }
   },
   "outputs": [],
   "source": [
    "glove = GloVe(name=\"6B\", dim=300)\n",
    "embedding_matrix = glove.get_vecs_by_tokens(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Setting hyperparameters and bulding the network</a>\n",
    "(<a href=\"#0\">Go to top</a>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will set our parameters like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.868989Z",
     "start_time": "2021-01-09T05:04:29.866241Z"
    }
   },
   "outputs": [],
   "source": [
    "# Size of the state vectors\n",
    "hidden_size = 128\n",
    "\n",
    "# General NN training parameters\n",
    "learning_rate = 0.0001\n",
    "epochs = 15\n",
    "\n",
    "# Embedding vector and vocabulary sizes\n",
    "embed_size = 300  # glove.6B.300d.txt\n",
    "vocab_size = len(vocab.itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to put our data into correct format before the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is made of these layers:\n",
    "* Embedding layer: This is where our words/tokens are mapped to word vectors.\n",
    "* RNN layer: We are using a simple RNN model. We stack 2 RNN layers in this example. More details about the RNN are available [here](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).\n",
    "* Linear layer: A linear layer with a single neuron is used to output the `isPositive` prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.892791Z",
     "start_time": "2021-01-09T05:04:29.881808Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=1)\n",
    "        self.rnn = nn.RNN(\n",
    "            embed_size, hidden_size, num_layers=num_layers, batch_first=True\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, 1)\n",
    "        self.act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeddings = self.embedding(inputs)\n",
    "        # Call the RNN layer\n",
    "        outputs, _ = self.rnn(embeddings)\n",
    "        \n",
    "        # Output shape after RNN: (batch_size, max_len, hidden_size)\n",
    "        # Get the output from the last time step with outputs[:, -1, :] below\n",
    "        # The output shape becomes: (batch_size, 1, hidden_size)\n",
    "        # Send it to the linear layer\n",
    "        outs = self.linear(outputs[:, -1, :])\n",
    "        return self.act(outs)\n",
    "    \n",
    "# Initialize the weights\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    if type(m) == nn.RNN:\n",
    "        for param in m._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(m._parameters[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's initialize this network. Then, we will need to make the embedding layer use our GloVe word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.902048Z",
     "start_time": "2021-01-09T05:04:29.899284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Our architecture with 2 RNN layers\n",
    "model = Net(vocab_size, embed_size, hidden_size, num_layers=2)\n",
    "\n",
    "# We set the embedding layer's parameters from GloVe\n",
    "model.embedding.weight.data.copy_(embedding_matrix)\n",
    "# We won't change/train the embedding layer\n",
    "model.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. <a name=\"8\">Training the network</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now, it is time to start our training. We define the loss function and training algorithm first. Then, training starts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define the trainer and loss function below. \n",
    "\n",
    "__Binary cross-entropy loss__ is used as this is a binary classification problem.\n",
    "\n",
    "$$\n",
    "\\mathrm{BinaryCrossEntropyLoss} = -\\sum_{examples}{(y\\log(p) + (1 - y)\\log(1 - p))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:04:29.906415Z",
     "start_time": "2021-01-09T05:04:29.903716Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setting our trainer\n",
    "trainer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# We will use Binary Cross-entropy loss\n",
    "# reduction=\"sum\" sums the losses for given output and target\n",
    "cross_ent_loss = nn.BCELoss(reduction=\"sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is time to start the training process. We will print the Binary cross-entropy loss loss after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-09T05:06:35.434926Z",
     "start_time": "2021-01-09T05:04:29.908071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train_loss 0.6261530711348094. Val_loss 0.570370820590428. Seconds 20.90886092185974\n",
      "Epoch 1. Train_loss 0.5600190313278682. Val_loss 0.5283386766910553. Seconds 21.350273847579956\n",
      "Epoch 2. Train_loss 0.5333711538806795. Val_loss 0.5153359385899136. Seconds 20.032791137695312\n",
      "Epoch 3. Train_loss 0.5150970947174799. Val_loss 0.502014913218362. Seconds 20.28631567955017\n",
      "Epoch 4. Train_loss 0.5012848254990956. Val_loss 0.49316429955618724. Seconds 20.181519746780396\n",
      "Epoch 5. Train_loss 0.48963918178800553. Val_loss 0.4873343655041286. Seconds 20.40234661102295\n",
      "Epoch 6. Train_loss 0.480525227009304. Val_loss 0.48011997972215925. Seconds 20.62388277053833\n",
      "Epoch 7. Train_loss 0.47264597958988613. Val_loss 0.47484635250908985. Seconds 20.39682173728943\n",
      "Epoch 8. Train_loss 0.46557222362548584. Val_loss 0.4719020448412214. Seconds 20.268911361694336\n",
      "Epoch 9. Train_loss 0.4582683128780789. Val_loss 0.4691456917354039. Seconds 22.037943124771118\n",
      "Epoch 10. Train_loss 0.45286133622366287. Val_loss 0.4677794202736446. Seconds 21.4872624874115\n",
      "Epoch 11. Train_loss 0.44789990663528445. Val_loss 0.46694758227893285. Seconds 20.344207048416138\n",
      "Epoch 12. Train_loss 0.4429518944876535. Val_loss 0.46557106426783973. Seconds 20.900564908981323\n",
      "Epoch 13. Train_loss 0.4530274767913516. Val_loss 0.46369833878108435. Seconds 20.85407829284668\n",
      "Epoch 14. Train_loss 0.4350993114993686. Val_loss 0.4624861376626151. Seconds 20.968944787979126\n"
     ]
    }
   ],
   "source": [
    "# Get the compute device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.apply(init_weights)\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    training_loss = 0\n",
    "    val_loss = 0\n",
    "    # Training loop, train the network\n",
    "    for data, target in train_loader:\n",
    "        trainer.zero_grad()\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(data)\n",
    "        L = cross_ent_loss(output, target.unsqueeze(1))\n",
    "        training_loss += L.item()\n",
    "        L.backward()\n",
    "        trainer.step()\n",
    "\n",
    "    # Validate the network, no training (no weight update)\n",
    "    for data, target in val_loader:\n",
    "        val_predictions = model(data.to(device))\n",
    "        L = cross_ent_loss(val_predictions, target.to(device).unsqueeze(1))\n",
    "        val_loss += L.item()\n",
    "\n",
    "    # Let's take the average losses\n",
    "    training_loss = training_loss / len(train_label)\n",
    "    val_loss = val_loss / len(val_label)\n",
    "\n",
    "    end = time.time()\n",
    "    print(\n",
    "        f\"Epoch {epoch}. Train_loss {training_loss}. Val_loss {val_loss}. Seconds {end-start}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. <a name=\"9\">Test the classifier on the validation data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's get the validation predictions. Earlier we made predictions on the validation set with this line: ```model(data.to(device))```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "val_predictions = []\n",
    "for data, target in val_loader:\n",
    "    val_preds = model(data.to(device))\n",
    "    val_predictions.extend(\n",
    "        [np.rint(val_pred)[0] for val_pred in val_preds.detach().cpu().numpy()]\n",
    "    )\n",
    "print(val_predictions[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix, classification report and accuracy score are printed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1327  725]\n",
      " [ 490 3058]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.65      0.69      2052\n",
      "         1.0       0.81      0.86      0.83      3548\n",
      "\n",
      "    accuracy                           0.78      5600\n",
      "   macro avg       0.77      0.75      0.76      5600\n",
      "weighted avg       0.78      0.78      0.78      5600\n",
      "\n",
      "Accuracy (validation): 0.7830357142857143\n"
     ]
    }
   ],
   "source": [
    "# Use the fitted pipeline to make predictions on the validation dataset\n",
    "print(confusion_matrix(val_label, val_predictions))\n",
    "print(classification_report(val_label, val_predictions))\n",
    "print(\"Accuracy (validation):\", accuracy_score(val_label, val_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This score isn't an improvement over the single layer network from yesterday. RNNs usually require more data than regular neural networks in training. They also have additional hyperparameters to work with: __max_len, hidden_size, embed_size__ \n",
    "\n",
    "We will see some improved versions of RNNs namely [Gated Recurrent Units](https://pytorch.org/docs/1.9.1/generated/torch.nn.GRU.html) and [Long Sort-term Memory Networks](https://pytorch.org/docs/1.9.1/generated/torch.nn.LSTM.html) tomorrow. Those approaches will have a higher chance of success. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. <a name=\"10\">Test the classifier on the unseen test data</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's get the test predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"../../data/examples/NLP-REVIEW-DATA-CLASSIFICATION-TEST.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = df_test[\"reviewText\"].fillna(value=\"missing\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TensorDataset(pad_features(test_text, max_len)) #, torch.tensor(val_label))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "test_predictions = []\n",
    "for data, in test_loader:\n",
    "    test_preds = model(data.to(device))\n",
    "    test_predictions.extend(\n",
    "        [np.rint(test_pred)[0] for test_pred in test_preds.detach().cpu().numpy()]\n",
    "    )\n",
    "print(test_predictions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"ID\"] = df_test[\"ID\"]\n",
    "result_df[\"isPositive\"] = test_predictions\n",
    "\n",
    "result_df.to_csv(\"result_day2_rnn.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. <a name=\"11\">Improvement ideas</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We can improve our model by\n",
    "* Changing hyper-parameters: Learning rate, batch size and hidden size\n",
    "* Increase the number of layers: num_layers\n",
    "* (Tomorrow) Switching to [Gated Recurrent Units](https://pytorch.org/docs/1.9.1/generated/torch.nn.GRU.html) and [Long Sort-term Memory Networks](https://pytorch.org/docs/1.9.1/generated/torch.nn.LSTM.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
